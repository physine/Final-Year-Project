{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beba100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2021-12-02 21:32:22,528 - utils - NumExpr defaulting to 6 threads.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feccda7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ml3d/configs/pointpillars_kitti.yml not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6d6f935e50e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcfg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ml3d/configs/pointpillars_kitti.yml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ml3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPointPillars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/open3d/_ml3d/utils/config.py\u001b[0m in \u001b[0;36mload_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'File {filename} not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ml3d/configs/pointpillars_kitti.yml not found"
     ]
    }
   ],
   "source": [
    "\n",
    "# === open3d === #\n",
    "\n",
    "import os\n",
    "import open3d.ml as _ml3d\n",
    "#import open3d.ml.torch as ml3d\n",
    "\n",
    "cfg_file = \"ml3d/configs/pointpillars_kitti.yml\"\n",
    "cfg = _ml3d.utils.Config.load_from_file(cfg_file)\n",
    "\n",
    "model = ml3d.models.PointPillars(**cfg.model)\n",
    "cfg.dataset['dataset_path'] = \"/path/to/your/dataset\"\n",
    "dataset = ml3d.datasets.KITTI(cfg.dataset.pop('dataset_path', None), **cfg.dataset)\n",
    "pipeline = ml3d.pipelines.ObjectDetection(model, dataset=dataset, device=\"gpu\", **cfg.pipeline)\n",
    "\n",
    "... \n",
    "# run inference on a single example.\n",
    "result = pipeline.run_inference(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795a2de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/physine/.cache/torch/hub/ultralytics_yolov5_master\n",
      "INFO - 2021-12-02 21:32:31,465 - torch_utils - YOLOv5 ðŸš€ 2021-11-3 torch 1.7.1 CUDA:0 (NVIDIA GeForce GTX 1060 3GB, 3011.375MB)\n",
      "\n",
      "INFO - 2021-12-02 21:32:48,109 - yolo - Fusing layers... \n",
      "INFO - 2021-12-02 21:32:48,713 - torch_utils - Model Summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "INFO - 2021-12-02 21:32:48,719 - yolo - Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === pytorch - yolo === #\n",
    "\n",
    "#from IPython.display import display\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "\n",
    "#config_path = \"/usr/local/lib/python3.8/dist-packages/yolo/yolov3.cfg\" # the YOLO net weights file\n",
    "#weights_path = \"/usr/local/lib/python3.8/dist-packages/yolo/yolov3.weights\" # weights_path = \"weights/yolov3-tiny.weights\"\n",
    "#labels_path = \"/usr/local/lib/python3.8/dist-packages/yolo/data/coco.names\"\n",
    "img_path2 = \"/home/physine/Documents/FYP/images/original.jpg\" # crowded sence\n",
    "img_path1 = \"/home/physine/Documents/FYP/images/single_biker.jpg\" # single biker\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "def run_obj_detect_model(img):\n",
    "    object_list = []\n",
    "    results = model(img)\n",
    "    df = results.pandas().xyxy[0]\n",
    "    t0 = time.time()\n",
    "    for index, row in df.iterrows():\n",
    "        # xmin,   ymin,   xmax,   ymax,   confidence, objectName\n",
    "        # row[0], row[1], row[2], row[3], row[4],     row[6]\n",
    "        object_list.append(row)      \n",
    "    print(f'infrance time of img: {time.time()-t0}')\n",
    "    return object_list\n",
    "     \n",
    "def array_to_img(array):\n",
    "    return Image.fromarray(array)\n",
    "    \n",
    "imgs = [\n",
    "    \"/home/physine/Documents/FYP/images/original.jpg\",\n",
    "    \"/home/physine/Documents/FYP/images/single_biker.jpg\"\n",
    "]\n",
    "\n",
    "# for img in imgs:\n",
    "#     print('===========================================================')\n",
    "#     objects = run_model(img)\n",
    "#     for obj in objects:\n",
    "#         print('-------------------------------------------------------------')\n",
    "#         print(f'xmin =\\t{obj[0]} \\nymin =\\t{obj[1]} \\nxmax =\\t{obj[2]} \\nymax =\\t{obj[3]} \\nconfidence =\\t{obj[4]} \\nobjectName =\\t{obj[6]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e696f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only ably to keep track of one object for each type of class \n",
    "\n",
    "class classHandeler:    \n",
    "    \n",
    "    # takes a seg map    \n",
    "    def update_occupied_pixels():\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d1db1b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.503000020980835\n",
      "0.484000027179718\n",
      "0.0\n",
      "0.7320000529289246\n",
      "1.1820000410079956\n",
      "1.1660000085830688\n",
      "1.222000002861023\n",
      "1.2070000171661377\n",
      "1.222000002861023\n",
      "1.2120000123977661\n",
      "1.227000117301941\n",
      "1.2420001029968262\n",
      "1.2650001049041748\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === pyrealsense2 === #\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "#from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "# transforms.ToTensor()\n",
    "### ===================== stuff used for Mask R-CNN (start) ===================== ###\n",
    "\n",
    "label_map = [\n",
    "               (0, 0, 0),  # background 0\n",
    "               (128, 0, 0), # aeroplane 1\n",
    "               (0, 128, 0), # bicycle 2 \n",
    "               (128, 128, 0), # bird 3\n",
    "               (0, 0, 128), # boat 4\n",
    "               (128, 0, 128), # bottle 5\n",
    "               (0, 128, 128), # bus 6\n",
    "               (128, 128, 128), # car 7\n",
    "               (64, 0, 0), # cat\n",
    "               (192, 0, 0), # chair\n",
    "               (64, 128, 0), # cow\n",
    "               (192, 128, 0), # dining table\n",
    "               (64, 0, 128), # dog\n",
    "               (192, 0, 128), # horse\n",
    "               (64, 128, 128), # motorbike\n",
    "               (192, 128, 128), # person 15\n",
    "               (0, 64, 0), # potted plant\n",
    "               (128, 64, 0), # sheep\n",
    "               (0, 192, 0), # sofa\n",
    "               (128, 192, 0), # train\n",
    "               (0, 64, 128) # tv/monitor 20\n",
    "]\n",
    "\n",
    "CAR = 7\n",
    "PERSON = 15\n",
    "TV = 20\n",
    "\n",
    "def draw_segmentation_map(outputs):\n",
    "    labels = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    ##print(labels)\n",
    "    ##print(labels.shape)\n",
    "    #print(type(labels)\n",
    "          \n",
    "    red_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    green_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    blue_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    \n",
    "    for label_num in range(0, len(label_map)):\n",
    "        index = labels == label_num\n",
    "        red_map[index] = np.array(label_map)[label_num, 0]\n",
    "        green_map[index] = np.array(label_map)[label_num, 1]\n",
    "        blue_map[index] = np.array(label_map)[label_num, 2]\n",
    "        \n",
    "    segmented_image = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "    return segmented_image\n",
    "\n",
    "def get_segment_labels(image, model, device):\n",
    "    # transform the image to tensor and load into computation device\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)\n",
    "    # uncomment the following lines for more info\n",
    "    # print(type(outputs))\n",
    "    # print(outputs['out'].shape)\n",
    "    # print(outputs)\n",
    "    return outputs\n",
    "\n",
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 0.6 # how much transparency to apply\n",
    "    beta = 1 - alpha # alpha + beta should equal 1\n",
    "    gamma = 0 # scalar added to each sum\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.addWeighted(segmented_image, alpha, image, beta, gamma, image)\n",
    "    return image\n",
    "\n",
    "### ===================== stuff used for Mask R-CNN (end) ===================== ###\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 6)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "color_path = 'V00P00A00C00_rgb.avi'\n",
    "depth_path = 'V00P00A00C00_depth.avi'\n",
    "colorwriter = cv2.VideoWriter(color_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (640,480), 5)\n",
    "depthwriter = cv2.VideoWriter(depth_path, cv2.VideoWriter_fourcc(*'XVID'), 30, (640,480), 5)\n",
    "\n",
    "#pipeline.start(config)\n",
    "\n",
    "profile = pipeline.start(config)\n",
    "# Get the sensor once at the beginning. (Sensor index: 1)\n",
    "sensor = pipeline.get_active_profile().get_device().query_sensors()[1]\n",
    "\n",
    "# Set the exposure anytime during the operation\n",
    "sensor.set_option(rs.option.exposure, 800.000)\n",
    "\n",
    "for _ in range(15):\n",
    "    import time\n",
    "    time.sleep(.05)\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    if not depth_frame or not color_frame:\n",
    "        continue\n",
    "        \n",
    "    #print(f'type(color_frame):\\t\\t{type(color_frame)}')\n",
    "    #print(f'type(color_frame.get_data():\\t{type(color_frame.get_data())}')\n",
    "    #img = cv2.cvtColor(cv2.imread(f'demo_image.jpeg'), cv2.COLOR_BGR2RGB)\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    #print(f'\\ntype(color_image):\\t\\t{type(color_image)}')\n",
    "    #print(f'color_image.shape:\\t\\t{color_image.shape}')\n",
    "    img = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "    #print(f'\\ntype(img):\\t\\t\\t{type(img)}')\n",
    "    #print(f'img.shape:\\t\\t\\t{img.shape}')\n",
    "    color_image = Image.fromarray(img)\n",
    "    #color_image = Image.fromarray(img)\n",
    "    #print(f'\\ntype(color_image):\\t\\t{type(color_image)}')\n",
    "    #print(f'color_image.shape:\\t\\t{color_image.shape}')\n",
    "    \n",
    "    # =============== Using YOLO =============== #\n",
    "    '''\n",
    "    infrance = run_obj_detect_model(color_image)    \n",
    "    for obj in infrance:\n",
    "        print('-------------------------------------------------------------')\n",
    "        print(f'xmin =\\t{obj[0]} \\nymin =\\t{obj[1]} \\nxmax =\\t{obj[2]} \\nymax =\\t{obj[3]} \\nconfidence =\\t{obj[4]} \\nobjectName =\\t{obj[6]}')\n",
    "    display(color_image)    \n",
    "    '''\n",
    "    # =============== Using Mask R-CNN =============== #    \n",
    "        \n",
    "    model = fcn_resnet50(pretrained=True, progress=False)\n",
    "    model = model.eval()\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    image_tensor = convert_tensor(color_image)\n",
    "    batch_int = torch.stack([image_tensor])\n",
    "    batch = convert_image_dtype(batch_int, dtype=torch.float)  \n",
    "    normalized_batch = F.normalize(batch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    outputs = model(normalized_batch)['out']\n",
    "    \n",
    "    if _ < 2:\n",
    "        continue    \n",
    "    \n",
    "    labels = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "    #print(labels.shape)\n",
    "    #print(outputs.shape)\n",
    "    \n",
    "    #print('\\nCAR')\n",
    "    #print(outputs[0, CAR, 220:260, 320:330])\n",
    "    \n",
    "    #print('\\nPERSON')\n",
    "    #print(outputs[0, PERSON, 220:260, 320:330])      \n",
    "    \n",
    "    segimg = draw_segmentation_map(outputs)\n",
    "    \n",
    "    #TODO: return an instance of a class used to manage \n",
    "    \n",
    "    #print(type(segimg))\n",
    "    #print(type(segimg.shape))\n",
    "    #print(segimg[2])\n",
    "    final_image = image_overlay(color_image, segimg)\n",
    "    #print(type(final_image))\n",
    "    ##display(Image.fromarray(final_image))\n",
    "    \n",
    "    #print(depth_frame.shape)\n",
    "    \n",
    "    # get_distance(x: int, y: int) â†’ float\n",
    "    print(depth_frame.get_distance(110, 110))\n",
    "    #depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    #print(depth_image[0:50, 0:15])\n",
    "    \n",
    "print('OK')\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1decfb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed0356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03688c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fb821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
